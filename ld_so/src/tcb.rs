// ld_so/src/tcb.rs
//! Thread Control Block and Dynamic TLS Resolution.
//! 
//! Implements __tls_get_addr and DTV management.
//! Supports both Static TLS (startup) and Dynamic TLS (dlopen).

use alloc::vec::Vec;
use core::{mem, ptr, slice};
use crate::header::elf;
use crate::tls::{get_tls_module, max_module_id, TLS_GENERATION, TlsModule};

/// The argument passed to `__tls_get_addr`.
/// Generated by the compiler for GD (General Dynamic) and LD (Local Dynamic) models.
#[repr(C)]
pub struct TlsIndex {
    /// Module ID (1-based index).
    pub ti_module: usize,
    /// Offset into the module's TLS block.
    pub ti_offset: usize,
}

/// The Dynamic Thread Vector (DTV).
/// Holds pointers to TLS blocks for every module.
#[repr(C)]
pub struct Dtv {
    /// Current capacity (number of modules).
    pub capacity: usize,
    /// The actual vector of pointers.
    /// Conceptually: `dtv[0]` is generation, `dtv[i]` is pointer to module i.
    /// However, the TCB points to `dtv + 1` (or `dtv + 2` depending on implementation).
    /// We maintain a simpler internal structure where `vector[0]` is gen.
    pub vector: *mut usize,
}

#[repr(C)]
pub struct Tcb {
    pub tcb_self: *mut Tcb,
    pub dtv: *mut Dtv,
    pub sysinfo: usize,
    pub stack_guard: usize,
    pub locale: usize,
}

impl Tcb {
    /// Create a new TCB for the current thread.
    /// Allocates the Static TLS block + TCB in one go.
    pub unsafe fn new(static_tls_size: usize, static_tls_align: usize) -> *mut Tcb {
        let tcb_size = mem::size_of::<Tcb>();
        
        // Variant II (x86_64): [ TLS Block ] [ TCB ]
        #[cfg(target_arch = "x86_64")]
        let (layout_size, tcb_offset) = {
            let size = static_tls_size + tcb_size;
            // Align the TCB start if necessary, though FS usually sets base.
            (size, static_tls_size)
        };

        // Variant I (RISC-V, AArch64): [ TCB ] [ TLS Block ]
        #[cfg(any(target_arch = "aarch64", target_arch = "riscv64"))]
        let (layout_size, tcb_offset) = {
            // Align TCB size so TLS block starts aligned
            let tcb_aligned = (tcb_size + static_tls_align - 1) & !(static_tls_align - 1);
            (tcb_aligned + static_tls_size, 0)
        };

        let ptr = crate::__rust_alloc(layout_size, static_tls_align);
        if ptr.is_null() { return ptr::null_mut(); }
        
        ptr::write_bytes(ptr, 0, layout_size);

        let tcb = ptr.add(tcb_offset) as *mut Tcb;
        
        (*tcb).tcb_self = tcb;
        // Initialize DTV with initial capacity based on currently loaded modules
        (*tcb).dtv = Dtv::new(max_module_id());
        (*tcb).stack_guard = 0xDEADBEEF; // Should be random

        tcb
    }

    pub unsafe fn activate(&self) {
        let ptr = self as *const Tcb as usize;
        #[cfg(target_arch = "x86_64")]
        core::arch::asm!("wrfsbase {}", in(reg) ptr);
        #[cfg(target_arch = "aarch64")]
        core::arch::asm!("msr tpidr_el0, {}", in(reg) ptr);
        #[cfg(target_arch = "riscv64")]
        core::arch::asm!("mv tp, {}", in(reg) ptr);
    }
}

impl Dtv {
    unsafe fn new(capacity: usize) -> *mut Dtv {
        let dtv_struct = crate::__rust_alloc(mem::size_of::<Dtv>(), mem::align_of::<Dtv>()) as *mut Dtv;
        
        // Allocate vector: [Generation, Ptr1, Ptr2, ... PtrN]
        let vec_len = capacity + 1;
        let vec_size = vec_len * mem::size_of::<usize>();
        let vec_ptr = crate::__rust_alloc_zeroed(vec_size, mem::align_of::<usize>()) as *mut usize;
        
        *vec_ptr = TLS_GENERATION.load(core::sync::atomic::Ordering::SeqCst);

        (*dtv_struct).capacity = capacity;
        (*dtv_struct).vector = vec_ptr;
        
        dtv_struct
    }

    unsafe fn resize(&mut self, new_capacity: usize) {
        if new_capacity <= self.capacity {
            return;
        }

        let vec_len = new_capacity + 1;
        let vec_size = vec_len * mem::size_of::<usize>();
        let new_vec = crate::__rust_alloc_zeroed(vec_size, mem::align_of::<usize>()) as *mut usize;

        // Copy existing pointers
        ptr::copy_nonoverlapping(self.vector, new_vec, self.capacity + 1);

        // Update generation
        *new_vec = TLS_GENERATION.load(core::sync::atomic::Ordering::SeqCst);

        // In a real impl, free self.vector here.
        self.vector = new_vec;
        self.capacity = new_capacity;
    }
}

/// The Runtime Entry Point for Dynamic TLS.
/// Called by code using `__thread` or `thread_local!` when the linker used the GD model.
#[no_mangle]
pub unsafe extern "C" fn __tls_get_addr(ti: *const TlsIndex) -> *mut u8 {
    let mod_id = (*ti).ti_module;
    let offset = (*ti).ti_offset;

    // 1. Get Thread Pointer
    let tcb = current_tcb();
    let dtv = &mut *(*tcb).dtv;

    // 2. Check Generation (Lazy DTV Update)
    let global_gen = TLS_GENERATION.load(core::sync::atomic::Ordering::Acquire);
    let dtv_gen = *dtv.vector;

    // If DTV is stale or too small for this module ID
    if dtv_gen < global_gen || mod_id > dtv.capacity {
        let required = max_module_id().max(mod_id);
        dtv.resize(required);
    }

    // 3. Retrieve Block Pointer
    // dtv.vector[mod_id] holds the pointer.
    let slot_ptr = dtv.vector.add(mod_id);
    let mut tls_block = *slot_ptr;

    // 4. Lazy Allocation
    // If the pointer is 0 (UNALLOCATED), we must allocate it now.
    if tls_block == 0 {
        tls_block = allocate_tls_module(mod_id);
        *slot_ptr = tls_block;
    }

    // 5. Return Address
    // 
    (tls_block as *mut u8).add(offset)
}

/// Allocates and initializes a TLS block for a specific module.
#[inline(never)]
unsafe fn allocate_tls_module(mod_id: usize) -> usize {
    let info = get_tls_module(mod_id).expect("TLS module not registered");

    // If this module is part of the Static TLS set (loaded at startup),
    // we shouldn't be allocating it dynamically unless something went wrong 
    // or we are in a thread created *before* this module was loaded (complex case).
    // However, for full dynamic TLS (dlopen), we do this:

    let ptr = crate::__rust_alloc(info.size, info.align);
    if ptr.is_null() {
        core::intrinsics::abort(); 
    }

    // Initialize .tdata
    if info.image_size > 0 {
        ptr::copy_nonoverlapping(info.image as *const u8, ptr, info.image_size);
    }
    
    // Zero .tbss
    if info.size > info.image_size {
        ptr::write_bytes(
            ptr.add(info.image_size), 
            0, 
            info.size - info.image_size
        );
    }

    ptr as usize
}

#[inline(always)]
unsafe fn current_tcb() -> *mut Tcb {
    #[cfg(target_arch = "x86_64")]
    {
        let tcb: *mut Tcb;
        core::arch::asm!("mov {}, fs:0", out(reg) tcb);
        tcb
    }
    #[cfg(target_arch = "aarch64")]
    {
        let tcb: *mut Tcb;
        core::arch::asm!("mrs {}, tpidr_el0", out(reg) tcb);
        tcb
    }
    #[cfg(target_arch = "riscv64")]
    {
        let tcb: *mut Tcb;
        core::arch::asm!("mv {}, tp", out(reg) tcb);
        tcb
    }
}